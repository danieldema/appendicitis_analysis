In this project I analyzed data of children admitted to Childrenâ€™s Hospital St. Hedwig in Regensburg, Germany, who are suspected of having appendicitis, with the goal of assessing and comparing performance of different models (logistic regression, decision tree, random forest) in predicting diagnosis. The source of the data is Kaggle: https://www.kaggle.com/datasets/joebeachcapital/regensburg-pediatric-appendicitis/data

# Initial Analysis

In app_analysis, I used pandas to clean the data by isolating for the following features: 'Diagnosis_Presumptive', 'Diagnosis', 'Alvarado_Score', 'Appendix_Diameter', 'Lower_Right_Abd_Pain', 'Contralateral_Rebound_Tenderness', 'Coughing_Pain', 'Nausea', 'Loss_of_Appetite', 'Body_Temperature', 'Stool', 'Psoas_Sign'. Rows with blank entries in any of these columns were dropped. Patients who tested positive and negative for appendicitis were separated into different dataframes. Proportion of false negatives was calculated to be 2.5% and proportion of false positives was calculated to be approimately 27%. Mean values for Alvarado score, appendix diameter, and body temperature were visually compared between the two groups through a bar graph built using Matplotlib. Count of patients with lower right abdomen pain, rebound tenderness, coughing pain, nausea, loss of appetite, body temperature, and presence of psoas sign were also compared through a bar graph.

# Logistic Regression

In app_logreg.ipynb, I used scikit-learn to build logistic regression models to predict diagnosis based on symptoms and clinical signs. Model 1 uses all features mentioned above, and Model 2 uses the same features except for body temperature. Model 1 has 0.93 accuracy with an AUC score of 0.90 as shown on the ROC curve and Model 2 has 0.92 accuracy also with an AUC score of 0.90. Both models predict appendicitis with precision 0.95, while Model 1 predicts no appendicitis with precision 0.86 and Model 2 predicts no appendicitis with precision 0.82. Models 1 and 2 both use L2 penalty. Computing the correlation matrix revealed low correlation between features, with values ranging from -0.49 to 0.4. Loss of appetite and nausea had a correlation of 0.39, and all other correlations of magnitude larger than 0.22 involve Alvarado score. I built Model 3 and Model 4 respectively using L1 penalty and elastic net penalty with L1 ratio 0.5, and both models had a lower accuracy of 0.76.

# Decision Tree

In app_decisiontree.ipynb, I used scikit-learn to build a decision tree model to predict diagnosis using all features mentioned above. This model has 0.95 accuracy with an AUC score of 0.93 as shown on the ROC curve. The model predicts appendicitis with precision 0.97 and no appendicitis with precision 0.87. I used cross-validation through GridSearchCV to optimize the max tree depth, which improved the model accuracy to 0.96 with an AUC score of 0.95. Note that this improvement in the model only decreased the number of false positives but did not change the number of false negatives. This is likely due to the small sizes of these groups (3 false positives and 4 false negatives prior to optimization).

# Random Forest

In app_randomforest.ipynb, I used scikit-learn to build a random forest model to predict diagnosis using all features mentioned above. This model has 0.97 accuracy with an AUC score of 0.98 as shown on the ROC curve. The model predicts appendicitis with precision 1.00 and no appendicitis with precision 0.88. As shown in a feature importances bar plot, binary symptoms have minimal feature importance while continuous features of body temperature, Alvarado score, and appendix diameter have (in ascending order) high feature importance. Dropping all other features resulted in a model with a lower accuracy score of 0.94. Limiting the max depth to the optimal value found through the cross-validation for decision tree did not change any predictions. Cross-validation to optimize the number of estimators made no improvement in accuracy (likely due to the small dataset size).
