In this project I am analyzing data of children admitted to Childrenâ€™s Hospital St. Hedwig in Regensburg, Germany, who are suspected of having appendicitis, with the goal of assessing and comparing performance of different models (logistic regression, decision tree, random forest) in predicting diagnosis. The source of the data is Kaggle: https://www.kaggle.com/datasets/joebeachcapital/regensburg-pediatric-appendicitis/data

In app_analysis, I use pandas to clean the data by isolating for the following features: 'Diagnosis_Presumptive', 'Diagnosis', 'Alvarado_Score', 'Appendix_Diameter', 'Lower_Right_Abd_Pain', 'Contralateral_Rebound_Tenderness', 'Coughing_Pain', 'Nausea', 'Loss_of_Appetite', 'Body_Temperature', 'Stool', 'Psoas_Sign'. Rows with blank entries in any of these columns are dropped. Patients who test positive and negative for appendicitis are separated into different dataframes. Proportion of false negatives is calculated to be 2.5% and proportion of false positives is calculated to be approimately 27%. Mean values for Alvarado score, appendix diameter, and body temperature are visually compared between the two groups through a bar graph built using Matplotlib. Count of patients with lower right abdomen pain, rebound tenderness, coughing pain, nausea, loss of appetite, body temperature, and presence of psoas sign are also compared through a bar graph.

In app_logreg.ipynb, I use scikit-learn to build logistic regression models to predict diagnosis based on symptoms and clinical signs. Model 1 uses all features mentioned above, and Model 2 uses the same features except for body temperature. Model 1 has 0.93 accuracy with an AUC score of 0.90 as shown on the ROC curve and Model 2 has 0.92 accuracy also with an AUC score of 0.90. Both models predict appendicitis with precision 0.95, while Model 1 predicts no appendicitis with precision 0.86 and Model 2 predicts no appendicitis with precision 0.82. Models 1 and 2 both use L2 penalty. Computing the correlation matrix revealed low correlation between features, with values ranging from -0.49 to 0.4. Loss of appetite and nausea have a correlation of 0.39, and all other correlations of magnitude larger than 0.22 involve Alvarado score. I built Model 3 and Model 4 respectively using L1 penalty and elastic net penalty with L1 ratio 0.5, and both models had a lower accuracy of 0.76.

In app_decisiontree.ipynb, I use scikit-learn to build a decision tree model to predict diagnosis using all features mentioned above. This model has 0.95 accuracy with an AUC score of 0.93 as shown on the ROC curve. The model predicts appendicitis with precision 0.97 and no appendicitis with precision 0.87. I use cross-validation through GridSearchCV to optimize the max tree depth, which improves the model accuracy to 0.96 with an AUC score of 0.95. Note that this improvement in the model only decreased the number of false positives but did not change the number of false negatives. This is likely due to the small sizes of these groups (3 false positives and 4 false negatives prior to optimization).

In app_randomforest.ipynb, I use scikit-learn to build a random forest model to predict diagnosis using all features mentioned above. This model has 0.97 accuracy with an AUC score of 0.98 as shown on the ROC curve. The model predicts appendicitis with precision 1.00 and no appendicitis with precision 0.88. As shown in a feature importances bar plot, binary symptoms have minimal feature importance while continuous features of body temperature, Alvarado score, and appendix diameter have (in ascending order) high feature importance.
